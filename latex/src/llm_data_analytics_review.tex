\documentclass{article}


\title{From Text-to-SQL Benchmarks to LLM-Powered Data Analytics: An Overview of Recent Advances}
\author{Marczis Bálint}
\begin{document}

\maketitle

\begin{abstract}
The long-standing challenge of Text-to-SQL (translating human language into structured SQL queries) addresses the needs of users across many fields, such as business analytics, research, and public data management, where individuals must work with large datasets but lack SQL expertise. Early deep neural network approaches struggled with schema understanding and generalization, leading to the development of more advanced LLM-based solutions.
This paper summarizes four representative works in this area: the Spider and BIRD benchmarks, Zhu et al.’s comprehensive survey of LLM-based Text-to-SQL methods, and CoddLLM, a recent LLM model explicitly trained for data analytics. These studies illustrate the progression from traditional neural models toward domain-specialized LLMs capable of making data analysis more intuitive, accurate, and widely accessible.
\end{abstract}

\section{Introduction}

Natural language interfaces to databases (NLIDB) have long represented a key challenge in the intersection of data management and natural language processing. The goal is to allow users to express analytical intents in natural language and automatically translate them into executable structured queries such as SQL. Early work on semantic parsing provided foundational methods, but research in this field accelerated dramatically with the introduction of large-scale, standardized benchmarks and the recent emergence of large language models (LLMs). This paper outlines the evolution of this research direction through four works: Spider \cite{yu2018spider}, BIRD \cite{li2023bird}, Zhu et al.’s survey \cite{zhu2024llmtexttosqlsurvey}, and CoddLLM \cite{wang2025coddllm}.

The \textbf{Spider dataset} \cite{yu2018spider}, introduced in 2018, represented a turning point for Text-to-SQL research by providing the first large-scale, human-labeled benchmark designed to test a model’s ability to generalize across different database schemas. Unlike earlier datasets, Spider separated training and test databases, ensuring that models could not simply memorize query patterns. Containing over 10,000 natural language questions and 200 databases spanning diverse domains, Spider became the standard testbed for evaluating semantic parsing models. Its results demonstrated that most systems at the time—such as Seq2SQL and SQLNet—performed poorly when faced with unseen database schemas, revealing the field’s key challenge: robust schema understanding and logical reasoning across domains.

Building on this foundation, the \textbf{BIRD benchmark} \cite{li2023bird} expanded the Text-to-SQL task into real-world and large-scale contexts, introducing the first benchmark explicitly tailored to LLMs. Whereas Spider focused on synthetic, well-structured data, BIRD simulated the messiness of enterprise databases-featuring incomplete column names, denormalized schemas, and inconsistent metadata. With over 98,000 natural language questions and 12,000 databases, BIRD provided a more realistic environment to assess whether large language models could serve as practical database interfaces. This transition marked a shift from academic datasets to industrial-scale evaluation, bridging the gap between controlled benchmarks and real analytical use cases.

As research attention shifted toward LLM-based Text-to-SQL systems, comprehensive analyses of their strengths and weaknesses emerged. The survey by \textbf{Zhu et al. (2024)} \cite{zhu2024llmtexttosqlsurvey} synthesized this progress, offering a structured taxonomy and evaluation of LLM-enhanced Text-to-SQL approaches. The authors found that prompt-based LLM systems, while impressive in linguistic flexibility, often fail at key analytical reasoning tasks. Common issues include incorrect schema linking, flawed joins and aggregations, and hallucinated columns or tables. The survey emphasized that such failures stem from the lack of explicit data-structure understanding in current models and identified the need for domain-specific fine-tuning and structured data representation—highlighting the importance of moving beyond pure prompting strategies.

This motivation directly inspired the development of \textbf{CoddLLM} \cite{wang2025coddllm}, a 2025 work proposing an LLM explicitly post-trained for data analytics. Rather than relying solely on prompts, CoddLLM introduces a new data-centric “recipe” for fine-tuning LLMs using synthetic data generation around database manipulation and representation. The model, based on Mistral-NeMo-12B, is trained on tasks that bridge tables and text—such as schema creation and table-text translation—allowing it to internalize database reasoning and schema comprehension. To evaluate these capabilities, the authors released \textit{AnalyticsMMLU}, a new benchmark containing analytical reasoning questions and database-centric challenges. CoddLLM achieves significant performance gains over GPT-3.5 and GPT-4 on tasks like Text-to-SQL and table selection, establishing a new milestone toward foundation models specialized for data analytics.

Taken together, these four works illustrate the evolution of the Text-to-SQL and LLM-for-analytics landscape: from standardized benchmarking (Spider), through real-world scalability (BIRD), to diagnostic understanding of model limitations (Zhu et al.), and finally to purpose-built solutions (CoddLLM). The progression reflects a broader shift in data management research from viewing LLMs as mere text generators toward recognizing them as emerging analytical agents capable of understanding, reasoning about, and interacting with structured data. This trajectory defines one of the most active and promising intersections between natural language processing and information systems research today.

\bibliographystyle{plain}
\bibliography{references}

\end{document}